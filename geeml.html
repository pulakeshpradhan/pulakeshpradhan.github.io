<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Data Science Workflow</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.5.0/styles/atom-one-dark.min.css">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">
    <style>
        /* Main Styles */
        :root {
            --primary-color: #2563eb;
            --primary-dark: #1e40af;
            --secondary-color: #10b981;
            --accent-color: #f97316;
            --text-color: #1e293b;
            --bg-color: #f8fafc;
            --card-bg: #ffffff;
            --header-bg: #ffffff;
            --footer-bg: #1e293b;
            --footer-text: #f1f5f9;
            --code-bg: #1e293b;
            --border-color: #e2e8f0;
            --shadow-color: rgba(0, 0, 0, 0.1);
            --hover-color: #f1f5f9;
            --section-padding: 60px 0;
            --transition: all 0.3s ease;
            --success: #22c55e;
        }

        /* Dark Theme */
        body.dark-theme {
            --bg-color: #0f172a;
            --text-color: #f1f5f9;
            --card-bg: #1e293b;
            --header-bg: #1e293b;
            --code-bg: #0f172a;
            --border-color: #334155;
            --hover-color: #334155;
            --shadow-color: rgba(0, 0, 0, 0.3);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Inter', sans-serif;
            background-color: var(--bg-color);
            color: var(--text-color);
            line-height: 1.6;
            transition: var(--transition);
        }

        .container {
            width: 100%;
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 20px;
        }

        /* Header Styles */
        header {
            background-color: var(--header-bg);
            box-shadow: 0 4px 12px var(--shadow-color);
            position: sticky;
            top: 0;
            z-index: 1000;
            padding: 15px 0;
        }

        .header-content {
            display: flex;
            justify-content: space-between;
            align-items: center;
        }

        .logo h1 {
            font-size: 1.8rem;
            color: var(--primary-color);
            font-weight: 700;
            letter-spacing: -0.5px;
        }

        .theme-toggle {
            background: none;
            border: none;
            color: var(--text-color);
            font-size: 1.2rem;
            cursor: pointer;
            transition: var(--transition);
            padding: 8px 12px;
            border-radius: 8px;
        }

        .theme-toggle:hover {
            color: var(--primary-color);
            background-color: var(--hover-color);
        }

        /* Tab Navigation */
        .tabs {
            display: flex;
            justify-content: center;
            margin-top: 20px;
            border-bottom: 1px solid var(--border-color);
            overflow-x: auto;
            -webkit-overflow-scrolling: touch;
            scrollbar-width: none;
            gap: 4px;
            padding-bottom: 2px;
        }

        .tabs::-webkit-scrollbar {
            display: none;
        }

        .tab-button {
            padding: 12px 20px;
            background: var(--card-bg);
            border: 1px solid var(--border-color);
            border-bottom: 3px solid transparent;
            color: var(--text-color);
            font-size: 0.95rem;
            font-weight: 600;
            cursor: pointer;
            transition: var(--transition);
            white-space: nowrap;
            border-radius: 8px 8px 0 0;
        }

        .tab-button:hover {
            color: var(--primary-color);
            background-color: var(--hover-color);
        }

        .tab-button.active {
            color: var(--primary-color);
            border-bottom: 3px solid var(--primary-color);
            background-color: var(--hover-color);
        }

        /* Tab Content */
        .tab-content {
            display: none;
            padding: 30px 0;
        }

        .tab-content.active {
            display: block;
            animation: fadeIn 0.5s ease;
        }

        @keyframes fadeIn {
            from { opacity: 0; transform: translateY(10px); }
            to { opacity: 1; transform: translateY(0); }
        }

        /* Content Styles */
        .section-title {
            font-size: 2rem;
            margin-bottom: 30px;
            color: var(--primary-color);
            text-align: center;
            font-weight: 700;
            position: relative;
            padding-bottom: 15px;
        }

        .section-title::after {
            content: "";
            position: absolute;
            bottom: 0;
            left: 50%;
            transform: translateX(-50%);
            width: 80px;
            height: 4px;
            background: var(--primary-color);
            border-radius: 2px;
        }

        .subsection-title {
            font-size: 1.5rem;
            margin: 30px 0 15px;
            color: var(--secondary-color);
            font-weight: 600;
            border-left: 4px solid var(--secondary-color);
            padding-left: 12px;
        }

        .content-card {
            background-color: var(--card-bg);
            border-radius: 12px;
            box-shadow: 0 4px 12px var(--shadow-color);
            padding: 25px;
            margin-bottom: 25px;
            transition: var(--transition);
            border: 1px solid var(--border-color);
        }

        .content-card:hover {
            transform: translateY(-5px);
            box-shadow: 0 8px 24px var(--shadow-color);
        }

        /* Code Blocks */
        pre {
            background-color: var(--code-bg);
            border-radius: 8px;
            padding: 20px;
            overflow-x: auto;
            margin: 15px 0;
            position: relative;
            border: 1px solid var(--border-color);
        }

        code {
            font-family: 'Consolas', 'Monaco', monospace;
            font-size: 0.95rem;
            line-height: 1.5;
        }

        .code-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 12px;
            padding-bottom: 8px;
            border-bottom: 1px solid var(--border-color);
            color: #94a3b8;
            font-size: 0.9rem;
        }

        .copy-btn {
            position: absolute;
            top: 10px;
            right: 10px;
            background-color: var(--primary-color);
            color: white;
            border: none;
            border-radius: 6px;
            padding: 6px 12px;
            font-size: 0.8rem;
            cursor: pointer;
            opacity: 0;
            transition: var(--transition);
            display: flex;
            align-items: center;
            gap: 6px;
        }

        .copy-btn i {
            font-size: 0.8rem;
        }

        pre:hover .copy-btn {
            opacity: 1;
        }

        .copy-btn:hover {
            background-color: var(--primary-dark);
        }

        /* Toast notification */
        .toast {
            position: fixed;
            bottom: 20px;
            right: 20px;
            background: var(--success);
            color: white;
            padding: 12px 24px;
            border-radius: 8px;
            display: none;
            animation: slideIn 0.3s ease;
            z-index: 1001;
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.2);
        }

        @keyframes slideIn {
            from {
                transform: translateY(100%);
                opacity: 0;
            }
            to {
                transform: translateY(0);
                opacity: 1;
            }
        }

        /* Note blocks */
        .note {
            background: #fff7ed;
            border-left: 4px solid var(--accent-color);
            padding: 16px;
            margin: 20px 0;
            border-radius: 0 8px 8px 0;
        }

        /* Responsive Design */
        @media (max-width: 768px) {
            .tabs {
                justify-content: flex-start;
            }
            
            .tab-button {
                padding: 10px 15px;
                font-size: 0.85rem;
            }
            
            .section-title {
                font-size: 1.8rem;
            }
            
            .subsection-title {
                font-size: 1.3rem;
            }

            .content-card {
                padding: 20px;
            }
        }

        /* Back to top button */
        .back-to-top {
            position: fixed;
            bottom: 20px;
            right: 20px;
            background: var(--primary-color);
            color: white;
            width: 40px;
            height: 40px;
            border-radius: 50%;
            display: flex;
            justify-content: center;
            align-items: center;
            cursor: pointer;
            opacity: 0;
            transition: var(--transition);
            z-index: 999;
            box-shadow: 0 2px 10px rgba(0, 0, 0, 0.2);
        }

        .back-to-top.visible {
            opacity: 1;
        }

        .back-to-top:hover {
            background: var(--primary-dark);
            transform: translateY(-3px);
        }

        /* Line numbers for code */
        .line-numbers {
            counter-reset: line;
            padding-left: 3em;
            position: relative;
        }

        .line-numbers > code {
            position: relative;
        }

        .line-numbers > code::before {
            counter-increment: line;
            content: counter(line);
            position: absolute;
            left: -3em;
            width: 2.5em;
            text-align: right;
            color: #6b7280;
            border-right: 1px solid #4b5563;
            padding-right: 0.5em;
            user-select: none;
        }
    </style>
</head>
<body>
    <header>
        <div class="container">
            <div class="header-content">
                <div class="logo">
                    <h1>Data Science Workflow</h1>
                </div>
                <button class="theme-toggle" id="themeToggle">
                    <i class="fas fa-moon"></i>
                </button>
            </div>
            <div class="tabs">
                <button class="tab-button active" data-tab="tab1">Google Earth Engine</button>
                <button class="tab-button" data-tab="tab6">GEE to Pandas</button>
                <button class="tab-button" data-tab="tab2">Data Gathering & CSV</button>
                <button class="tab-button" data-tab="tab3">Exploratory Data Analysis</button>
                <button class="tab-button" data-tab="tab4">Feature Engineering</button>
                <button class="tab-button" data-tab="tab5">Machine Learning</button>
            </div>
        </div>
    </header>

    <main class="container">
        <!-- Tab 1: Google Earth Engine -->
        <div class="tab-content active" id="tab1">
            <h2 class="section-title">Processing and Downloading Images from Google Earth Engine</h2>
            
            <div class="content-card">
                <h3 class="subsection-title">Setting Up the Environment</h3>
                <p>To work with Google Earth Engine, you'll need to install the necessary Python packages and authenticate with your Google account.</p>
                
                <pre>
                    <div class="code-header">
                        <span>Environment Setup</span>
                    </div>
                    <button class="copy-btn"><i class="fas fa-copy"></i> Copy</button>
                    <code class="python">
# Install required packages
!pip install earthengine-api
!pip install geemap
!pip install folium

# Import libraries
import ee
import geemap
import folium
import os
import matplotlib.pyplot as plt
import numpy as np
from datetime import datetime
                    </code>
                </pre>
                
                <p>These packages provide the foundation for interacting with Earth Engine's API and visualizing geospatial data.</p>
            </div>
            
            <div class="content-card">
                <h3 class="subsection-title">Authenticating with Google Earth Engine</h3>
                <p>Before using Earth Engine, you need to authenticate your account:</p>
                
                <pre>
                    <div class="code-header">
                        <span>Authentication</span>
                    </div>
                    <button class="copy-btn"><i class="fas fa-copy"></i> Copy</button>
                    <code class="python">
# Initialize Earth Engine
try:
    ee.Initialize()
    print("Earth Engine already initialized")
except Exception as e:
    ee.Authenticate()
    ee.Initialize()
    print("Earth Engine initialized")
                    </code>
                </pre>
                
                <p>This authentication process will open a browser window where you'll need to sign in with your Google account that has Earth Engine access.</p>
            </div>
            
            <div class="content-card">
                <h3 class="subsection-title">Creating an Interactive Map</h3>
                <p>Use geemap to create interactive maps for visualizing Earth Engine data:</p>
                
                <pre>
                    <div class="code-header">
                        <span>Interactive Map</span>
                    </div>
                    <button class="copy-btn"><i class="fas fa-copy"></i> Copy</button>
                    <code class="python">
# Create an interactive map
Map = geemap.Map()

# Set center and zoom level
Map.setCenter(-122.085, 37.422, 10)  # Longitude, Latitude, Zoom level

# Display the map
Map
                    </code>
                </pre>
                
                <p>This creates an interactive map centered on the specified coordinates with the given zoom level.</p>
            </div>
            
            <div class="content-card">
                <h3 class="subsection-title">Working with Earth Engine Data Catalogs</h3>
                <p>Access satellite imagery from various sources like Landsat, Sentinel, and MODIS:</p>
                
                <pre>
                    <div class="code-header">
                        <span>Data Catalogs</span>
                    </div>
                    <button class="copy-btn"><i class="fas fa-copy"></i> Copy</button>
                    <code class="python">
# Load Landsat 8 Surface Reflectance collection
landsat = ee.ImageCollection('LANDSAT/LC08/C02/T1_L2')

# Filter by date and location
point = ee.Geometry.Point([-122.085, 37.422])
filtered = landsat.filterBounds(point) \
                  .filterDate('2020-01-01', '2020-12-31') \
                  .sort('CLOUD_COVER') \
                  .first()

# Print metadata
print(filtered.getInfo())
                    </code>
                </pre>
                
                <p>This code retrieves Landsat 8 imagery for a specific location and time period, sorting by cloud cover to get the clearest image.</p>
            </div>
            
            <div class="content-card">
                <h3 class="subsection-title">Downloading Images from Earth Engine</h3>
                <p>Export Earth Engine images to your local machine:</p>
                
                <pre>
                    <div class="code-header">
                        <span>Download Images</span>
                    </div>
                    <button class="copy-btn"><i class="fas fa-copy"></i> Copy</button>
                    <code class="python">
# Define region of interest
roi = ee.Geometry.Rectangle([-122.5, 37.2, -121.8, 37.8])

# Prepare the image for export (example with NDVI calculation)
ndvi = filtered.normalizedDifference(['SR_B5', 'SR_B4']).rename('NDVI')
image_to_export = ndvi.clip(roi)

# Get download URL
url = image_to_export.getDownloadURL({
    'scale': 30,  # 30 meters per pixel
    'region': roi,
    'format': 'GeoTIFF'
})

# Download the image
import requests
response = requests.get(url)
with open('landsat_ndvi.tif', 'wb') as f:
    f.write(response.content)
print("Image downloaded successfully")
                    </code>
                </pre>
                
                <p>This code calculates NDVI (Normalized Difference Vegetation Index) from Landsat bands and downloads the result as a GeoTIFF file.</p>
            </div>
        </div>

        <!-- Tab 6: GEE to Pandas (New Tab) -->
        <div class="tab-content" id="tab6">
            <h2 class="section-title">Converting Google Earth Engine Data to Pandas</h2>
            
            <div class="content-card">
                <h3 class="subsection-title">Authenticate Google Earth Engine</h3>
                <p>Before working with Earth Engine data, you need to authenticate and initialize your session:</p>
                
                <pre>
                    <div class="code-header">
                        <span>GEE Authentication</span>
                    </div>
                    <button class="copy-btn"><i class="fas fa-copy"></i> Copy</button>
                    <code class="python">
# Import required libraries 
import ee
import geemap
import pandas as pd
import matplotlib.pyplot as plt

# Authenticate and initialize Earth Engine
ee.Authenticate()
ee.Initialize(project="geepulakesh")

# Create an interactive map
Map = geemap.Map()
Map
                    </code>
                </pre>
                
                <div class="note">
                    <strong>Note:</strong> Replace <code>"geepulakesh"</code> with your actual GEE project ID. Run this step once per session.
                </div>
            </div>
            
            <div class="content-card">
                <h3 class="subsection-title">Converting ImageCollections to DataFrame</h3>
                <p>Extract time-series data from Earth Engine ImageCollections into pandas DataFrames:</p>
                
                <pre>
                    <div class="code-header">
                        <span>ImageCollection to DataFrame</span>
                    </div>
                    <button class="copy-btn"><i class="fas fa-copy"></i> Copy</button>
                    <code class="python">
# Define the forest FeatureCollection
forest = ee.FeatureCollection("projects/google/charts_feature_example").filter(
    ee.Filter.eq("label", "Forest")
)

# Load the MODIS vegetation indices dataset
veg_indices = (
    ee.ImageCollection("MODIS/061/MOD13A1")
    .filter(ee.Filter.date("2000-01-01", "2020-01-01"))
    .select(["NDVI", "EVI"])
)

# Extract dates from the ImageCollection
date_property = veg_indices.aggregate_array("system:time_start")
dates = pd.to_datetime(date_property.getInfo(), unit="ms")

# Function to reduce an image by regions
def reduce_image(image):
    return image.reduceRegions(
        collection=forest,
        reducer=ee.Reducer.mean(),
        scale=500
    ).select(["NDVI", "EVI", "system:time_start"])

# Map the function over the ImageCollection
reduced = veg_indices.map(reduce_image).flatten()

# Convert the results to a list of dictionaries
data = reduced.getInfo()
features = data['features']

# Convert the features list to a DataFrame
df = pd.DataFrame([f['properties'] for f in features])

# Add the extracted dates as a new column
df['Date'] = pd.to_datetime(dates)

# Display the resulting DataFrame
df

# Export the dataframe to a CSV file
# df.to_csv('imagecollection_df.csv', index=False)
                    </code>
                </pre>
                
                <p>This code extracts vegetation indices (NDVI and EVI) from MODIS satellite imagery for forest areas and converts them to a pandas DataFrame with proper date formatting.</p>
            </div>
            
            <div class="content-card">
                <h3 class="subsection-title">Converting Images to DataFrame</h3>
                <p>Extract spatial data from Earth Engine Images into pandas DataFrames:</p>
                
                <pre>
                    <div class="code-header">
                        <span>Image to DataFrame</span>
                    </div>
                    <button class="copy-btn"><i class="fas fa-copy"></i> Copy</button>
                    <code class="python">
# Load ecoregions FeatureCollection
ecoregions = ee.FeatureCollection("projects/google/charts_feature_example")

# Load the PRISM dataset and select temperature-related bands
image = ee.ImageCollection("OREGONSTATE/PRISM/Norm91m").toBands().select("[0-9][0-9]_tmean")

# Reduce the image by regions to calculate mean temperature for each region
reduced = image.reduceRegions(
    collection=ecoregions,
    reducer=ee.Reducer.mean(),
    scale=500
)

# Extract the results as a dictionary
data = reduced.getInfo()

# Convert the dictionary to a Pandas DataFrame
df = pd.DataFrame([f['properties'] for f in data['features']])

# Display the DataFrame
df

# Export the dataframe to a CSV file
# df.to_csv('image_df.csv', index=False)
                    </code>
                </pre>
                
                <p>This code extracts mean temperature data from the PRISM climate dataset for different ecoregions and converts it to a pandas DataFrame for further analysis.</p>
            </div>
            
            <div class="content-card">
                <h3 class="subsection-title">Converting FeatureCollections to DataFrame</h3>
                <p>Convert Earth Engine FeatureCollections directly to pandas DataFrames:</p>
                
                <pre>
                    <div class="code-header">
                        <span>FeatureCollection to DataFrame</span>
                    </div>
                    <button class="copy-btn"><i class="fas fa-copy"></i> Copy</button>
                    <code class="python">
# Define the ecoregions FeatureCollection
ecoregions = ee.FeatureCollection("projects/google/charts_feature_example")

# Select temperature-related properties
features = ecoregions.select("[0-9][0-9]_tmean|label")

# Convert the FeatureCollection to a DataFrame
df = geemap.ee_to_df(features)

# Display the DataFrame
df

# Export the dataframe to a CSV file
# df.to_csv('exported_df.csv', index=False)
                    </code>
                </pre>
                
                <p>This code uses geemap's <code>ee_to_df</code> function to directly convert an Earth Engine FeatureCollection to a pandas DataFrame, which is a more concise approach for simple conversions.</p>
            </div>
            
            <div class="content-card">
                <h3 class="subsection-title">Working with Converted DataFrames</h3>
                <p>Analyze and visualize the converted Earth Engine data using pandas and matplotlib:</p>
                
                <pre>
                    <div class="code-header">
                        <span>Analyzing Converted Data</span>
                    </div>
                    <button class="copy-btn"><i class="fas fa-copy"></i> Copy</button>
                    <code class="python">
# Assuming we have the DataFrame from the ImageCollection conversion
# Let's analyze and visualize the NDVI and EVI time series

# Basic statistics
print("NDVI Statistics:")
print(df['NDVI'].describe())
print("\nEVI Statistics:")
print(df['EVI'].describe())

# Time series plot
plt.figure(figsize=(12, 6))
plt.plot(df['Date'], df['NDVI'], 'g-', label='NDVI')
plt.plot(df['Date'], df['EVI'], 'b-', label='EVI')
plt.title('Vegetation Indices Over Time')
plt.xlabel('Date')
plt.ylabel('Index Value')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# Monthly averages
df['Month'] = df['Date'].dt.month
monthly_avg = df.groupby('Month')[['NDVI', 'EVI']].mean()

plt.figure(figsize=(10, 6))
monthly_avg.plot(kind='bar')
plt.title('Monthly Average Vegetation Indices')
plt.xlabel('Month')
plt.ylabel('Index Value')
plt.xticks(range(12), ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])
plt.tight_layout()
plt.show()

# Correlation analysis
correlation = df[['NDVI', 'EVI']].corr()
print("\nCorrelation between NDVI and EVI:")
print(correlation)
                    </code>
                </pre>
                
                <p>This code demonstrates how to analyze and visualize the Earth Engine data after converting it to a pandas DataFrame, including time series plotting, monthly averages, and correlation analysis.</p>
            </div>
        </div>

        <!-- Tab 2: Data Gathering & CSV -->
        <div class="tab-content" id="tab2">
            <h2 class="section-title">Data Gathering and CSV File Handling</h2>
            
            <div class="content-card">
                <h3 class="subsection-title">Reading CSV Files with Pandas</h3>
                <p>Pandas is the go-to library for reading and manipulating CSV data in Python:</p>
                
                <pre>
                    <div class="code-header">
                        <span>Basic CSV Reading</span>
                    </div>
                    <button class="copy-btn"><i class="fas fa-copy"></i> Copy</button>
                    <code class="python">
# Import pandas
import pandas as pd

# Read a CSV file
df = pd.read_csv('data.csv')

# Display first few rows
print(df.head())

# Get basic information about the dataframe
print(df.info())

# Get statistical summary
print(df.describe())
                    </code>
                </pre>
                
                <p>This code reads a CSV file into a pandas DataFrame and provides basic information about its contents.</p>
            </div>
            
            <div class="content-card">
                <h3 class="subsection-title">Advanced CSV Reading Techniques</h3>
                <p>Customize how pandas reads CSV files to handle various formats:</p>
                
                <pre>
                    <div class="code-header">
                        <span>Advanced CSV Reading</span>
                    </div>
                    <button class="copy-btn"><i class="fas fa-copy"></i> Copy</button>
                    <code class="python">
# Read CSV with custom options
df = pd.read_csv('complex_data.csv',
                 sep=';',  # Use semicolon as separator
                 decimal=',',  # Use comma as decimal point
                 encoding='latin1',  # Specify encoding
                 na_values=['NA', 'N/A', 'Missing'],  # Custom NA values
                 skiprows=2,  # Skip first 2 rows
                 nrows=1000,  # Read only 1000 rows
                 parse_dates=['Date'],  # Parse date columns
                 index_col='ID')  # Set index column

print(df.head())
                    </code>
                </pre>
                
                <p>This example shows how to handle CSV files with non-standard formats, such as different separators, decimal markers, and custom NA values.</p>
            </div>
            
            <div class="content-card">
                <h3 class="subsection-title">Reading Large CSV Files</h3>
                <p>Efficiently process large CSV files that don't fit in memory:</p>
                
                <pre>
                    <div class="code-header">
                        <span>Large CSV Processing</span>
                    </div>
                    <button class="copy-btn"><i class="fas fa-copy"></i> Copy</button>
                    <code class="python">
# Read large CSV in chunks
chunk_size = 100000
chunks = []

# Process the file in chunks
for chunk in pd.read_csv('large_data.csv', chunksize=chunk_size):
    # Process each chunk (e.g., filter rows)
    processed = chunk[chunk['value'] > 100]
    chunks.append(processed)
    
# Combine processed chunks
result = pd.concat(chunks)
print(f"Processed {len(result)} rows")

# Alternative: use dask for out-of-memory processing
import dask.dataframe as dd
ddf = dd.read_csv('large_data.csv')
result = ddf[ddf['value'] > 100].compute()
                    </code>
                </pre>
                
                <p>This code demonstrates two approaches for handling large CSV files: using pandas' chunking functionality and using dask for out-of-memory processing.</p>
            </div>
            
            <div class="content-card">
                <h3 class="subsection-title">Writing Data to CSV Files</h3>
                <p>Save processed data back to CSV format:</p>
                
                <pre>
                    <div class="code-header">
                        <span>CSV Export</span>
                    </div>
                    <button class="copy-btn"><i class="fas fa-copy"></i> Copy</button>
                    <code class="python">
# Basic CSV export
df.to_csv('output.csv', index=False)

# Advanced CSV export
df.to_csv('formatted_output.csv',
          sep=';',  # Use semicolon as separator
          decimal=',',  # Use comma as decimal point
          encoding='utf-8',  # Specify encoding
          date_format='%Y-%m-%d',  # Format dates
          float_format='%.2f',  # Format floats to 2 decimal places
          index=False,  # Don't write row indices
          columns=['ID', 'Name', 'Value'])  # Select columns to write
                    </code>
                </pre>
                
                <p>These examples show how to write DataFrames to CSV files with various formatting options.</p>
            </div>
        </div>

        <!-- Tab 3: Exploratory Data Analysis -->
        <div class="tab-content" id="tab3">
            <h2 class="section-title">Exploratory Data Analysis</h2>
            
            <div class="content-card">
                <h3 class="subsection-title">Data Overview and Cleaning</h3>
                <p>Start your EDA by examining and cleaning your dataset:</p>
                
                <pre>
                    <div class="code-header">
                        <span>Data Cleaning</span>
                    </div>
                    <button class="copy-btn"><i class="fas fa-copy"></i> Copy</button>
                    <code class="python">
# Import libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Load data
df = pd.read_csv('dataset.csv')

# Check data shape
print(f"Dataset shape: {df.shape}")

# Check for missing values
missing = df.isnull().sum()
print(f"Missing values per column:\n{missing}")

# Handle missing values
df_cleaned = df.copy()
# Fill numeric columns with median
numeric_cols = df.select_dtypes(include=['number']).columns
df_cleaned[numeric_cols] = df_cleaned[numeric_cols].fillna(df_cleaned[numeric_cols].median())
# Fill categorical columns with mode
cat_cols = df.select_dtypes(include=['object']).columns
for col in cat_cols:
    df_cleaned[col] = df_cleaned[col].fillna(df_cleaned[col].mode()[0])

# Check for duplicates
duplicates = df_cleaned.duplicated().sum()
print(f"Number of duplicate rows: {duplicates}")
# Remove duplicates
df_cleaned = df_cleaned.drop_duplicates()
                    </code>
                </pre>
                
                <p>This code provides a comprehensive approach to initial data examination and cleaning, handling missing values and duplicates.</p>
            </div>
            
            <div class="content-card">
                <h3 class="subsection-title">Statistical Analysis</h3>
                <p>Perform statistical analysis to understand your data distribution:</p>
                
                <pre>
                    <div class="code-header">
                        <span>Statistical Analysis</span>
                    </div>
                    <button class="copy-btn"><i class="fas fa-copy"></i> Copy</button>
                    <code class="python">
# Basic statistics
print(df_cleaned.describe(include='all').T)

# Correlation analysis
numeric_df = df_cleaned.select_dtypes(include=['number'])
correlation = numeric_df.corr()

# Plot correlation heatmap
plt.figure(figsize=(12, 10))
sns.heatmap(correlation, annot=True, cmap='coolwarm', linewidths=0.5)
plt.title('Correlation Matrix')
plt.tight_layout()
plt.show()

# Check for outliers using IQR method
def detect_outliers(df, column):
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]
    return outliers

# Check outliers for a numeric column
for col in numeric_cols:
    outliers = detect_outliers(df_cleaned, col)
    print(f"Column {col} has {len(outliers)} outliers")
                    </code>
                </pre>
                
                <p>This code performs statistical analysis including correlation analysis and outlier detection using the IQR method.</p>
            </div>
            
            <div class="content-card">
                <h3 class="subsection-title">Data Visualization</h3>
                <p>Create visualizations to better understand your data:</p>
                
                <pre>
                    <div class="code-header">
                        <span>Data Visualization</span>
                    </div>
                    <button class="copy-btn"><i class="fas fa-copy"></i> Copy</button>
                    <code class="python">
# Set plotting style
sns.set(style="whitegrid")

# Distribution plots
plt.figure(figsize=(15, 10))
for i, col in enumerate(numeric_cols[:4]):  # First 4 numeric columns
    plt.subplot(2, 2, i+1)
    sns.histplot(df_cleaned[col], kde=True)
    plt.title(f'Distribution of {col}')
plt.tight_layout()
plt.show()

# Categorical data visualization
plt.figure(figsize=(12, 8))
for i, col in enumerate(cat_cols[:2]):  # First 2 categorical columns
    plt.subplot(1, 2, i+1)
    counts = df_cleaned[col].value_counts()
    sns.barplot(x=counts.index, y=counts.values)
    plt.title(f'Count of {col}')
    plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# Scatter plot matrix
if len(numeric_cols) >= 4:
    sns.pairplot(df_cleaned[numeric_cols[:4]], height=2.5)
    plt.suptitle('Scatter Plot Matrix', y=1.02)
    plt.show()

# Box plots for detecting outliers
plt.figure(figsize=(15, 10))
for i, col in enumerate(numeric_cols[:4]):
    plt.subplot(2, 2, i+1)
    sns.boxplot(y=df_cleaned[col])
    plt.title(f'Box Plot of {col}')
plt.tight_layout()
plt.show()
                    </code>
                </pre>
                
                <p>This comprehensive visualization code creates distribution plots, categorical bar plots, scatter plot matrices, and box plots to help understand data patterns and identify outliers.</p>
            </div>
        </div>

        <!-- Tab 4: Feature Engineering -->
        <div class="tab-content" id="tab4">
            <h2 class="section-title">Feature Engineering</h2>
            
            <div class="content-card">
                <h3 class="subsection-title">Handling Categorical Variables</h3>
                <p>Transform categorical variables into numerical representations:</p>
                
                <pre>
                    <div class="code-header">
                        <span>Categorical Encoding</span>
                    </div>
                    <button class="copy-btn"><i class="fas fa-copy"></i> Copy</button>
                    <code class="python">
# Import libraries
import pandas as pd
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from category_encoders import TargetEncoder

# Sample data
df = pd.read_csv('data.csv')
cat_cols = df.select_dtypes(include=['object']).columns

# Method 1: Label Encoding
label_encoder = LabelEncoder()
for col in cat_cols:
    df[f'{col}_label'] = label_encoder.fit_transform(df[col])

# Method 2: One-Hot Encoding
df_onehot = pd.get_dummies(df, columns=cat_cols, drop_first=True)

# Method 3: Target Encoding (for supervised learning)
target_col = 'target'  # Replace with your target column
target_encoder = TargetEncoder()
df_target_encoded = df.copy()
df_target_encoded[cat_cols] = target_encoder.fit_transform(df[cat_cols], df[target_col])

# Method 4: Frequency Encoding
for col in cat_cols:
    frequency = df[col].value_counts(normalize=True).to_dict()
    df[f'{col}_freq'] = df[col].map(frequency)

print("Original shape:", df.shape)
print("One-hot encoded shape:", df_onehot.shape)
                    </code>
                </pre>
                
                <p>This code demonstrates four different methods for encoding categorical variables: Label Encoding, One-Hot Encoding, Target Encoding, and Frequency Encoding.</p>
            </div>
            
            <div class="content-card">
                <h3 class="subsection-title">Feature Scaling and Normalization</h3>
                <p>Scale numerical features to improve model performance:</p>
                
                <pre>
                    <div class="code-header">
                        <span>Feature Scaling</span>
                    </div>
                    <button class="copy-btn"><i class="fas fa-copy"></i> Copy</button>
                    <code class="python">
# Import libraries
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
from sklearn.model_selection import train_test_split

# Sample data
df = pd.read_csv('data.csv')
numeric_cols = df.select_dtypes(include=['number']).columns
X = df[numeric_cols]
y = df['target']  # Replace with your target column

# Split data to avoid data leakage
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Method 1: StandardScaler (Z-score normalization)
std_scaler = StandardScaler()
X_train_std = std_scaler.fit_transform(X_train)
X_test_std = std_scaler.transform(X_test)

# Method 2: MinMaxScaler (scale to range [0,1])
minmax_scaler = MinMaxScaler()
X_train_minmax = minmax_scaler.fit_transform(X_train)
X_test_minmax = minmax_scaler.transform(X_test)

# Method 3: RobustScaler (robust to outliers)
robust_scaler = RobustScaler()
X_train_robust = robust_scaler.fit_transform(X_train)
X_test_robust = robust_scaler.transform(X_test)

# Convert back to DataFrames for better visualization
X_train_std_df = pd.DataFrame(X_train_std, columns=numeric_cols)
X_train_minmax_df = pd.DataFrame(X_train_minmax, columns=numeric_cols)
X_train_robust_df = pd.DataFrame(X_train_robust, columns=numeric_cols)

# Compare results
print("Original data statistics:")
print(X_train.describe().round(2))
print("\nStandardScaled data statistics:")
print(X_train_std_df.describe().round(2))
print("\nMinMaxScaled data statistics:")
print(X_train_minmax_df.describe().round(2))
                    </code>
                </pre>
                
                <p>This code demonstrates three common scaling methods: StandardScaler (Z-score normalization), MinMaxScaler (scaling to [0,1]), and RobustScaler (scaling robust to outliers).</p>
            </div>
            
            <div class="content-card">
                <h3 class="subsection-title">Creating New Features</h3>
                <p>Generate new features to capture additional patterns in your data:</p>
                
                <pre>
                    <div class="code-header">
                        <span>Feature Creation</span>
                    </div>
                    <button class="copy-btn"><i class="fas fa-copy"></i> Copy</button>
                    <code class="python">
# Import libraries
import pandas as pd
import numpy as np
from datetime import datetime

# Sample data
df = pd.read_csv('data.csv', parse_dates=['date_column'])

# 1. Mathematical transformations
df['log_feature'] = np.log1p(df['numeric_feature'])  # log(1+x) to handle zeros
df['sqrt_feature'] = np.sqrt(df['numeric_feature'])
df['squared_feature'] = df['numeric_feature'] ** 2

# 2. Interaction features
df['interaction'] = df['feature1'] * df['feature2']
df['ratio'] = df['feature1'] / (df['feature2'] + 1e-8)  # Add small value to avoid division by zero
df['sum'] = df['feature1'] + df['feature2']
df['diff'] = df['feature1'] - df['feature2']

# 3. Date features
df['year'] = df['date_column'].dt.year
df['month'] = df['date_column'].dt.month
df['day'] = df['date_column'].dt.day
df['day_of_week'] = df['date_column'].dt.dayofweek
df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)
df['quarter'] = df['date_column'].dt.quarter

# 4. Binning continuous variables
df['age_group'] = pd.cut(df['age'], bins=[0, 18, 35, 50, 65, 100], 
                         labels=['0-18', '19-35', '36-50', '51-65', '65+'])

# 5. Aggregation features (assuming grouped data)
group_features = df.groupby('category').agg({
    'numeric_feature': ['mean', 'min', 'max', 'std']
}).reset_index()
group_features.columns = ['category', 'grp_mean', 'grp_min', 'grp_max', 'grp_std']
df = df.merge(group_features, on='category', how='left')

# 6. Polynomial features
df['poly_interaction'] = df['feature1']**2 * df['feature2']

print("Original number of features:", len(df.columns) - len(df.columns))
print("New number of features:", len(df.columns))
                    </code>
                </pre>
                
                <p>This comprehensive feature engineering code demonstrates creating new features through mathematical transformations, interactions, date feature extraction, binning, aggregation, and polynomial features.</p>
            </div>
            
            <div class="content-card">
                <h3 class="subsection-title">Dimensionality Reduction</h3>
                <p>Reduce the number of features while preserving information:</p>
                
                <pre>
                    <div class="code-header">
                        <span>Dimensionality Reduction</span>
                    </div>
                    <button class="copy-btn"><i class="fas fa-copy"></i> Copy</button>
                    <code class="python">
# Import libraries
import pandas as pd
import numpy as np
from sklearn.decomposition import PCA, TruncatedSVD
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler

# Sample data
df = pd.read_csv('data.csv')
numeric_cols = df.select_dtypes(include=['number']).columns
X = df[numeric_cols]

# Standardize the data
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Method 1: PCA
pca = PCA(n_components=2)  # Reduce to 2 dimensions
X_pca = pca.fit_transform(X_scaled)
print("Explained variance ratio:", pca.explained_variance_ratio_)
print("Total explained variance:", sum(pca.explained_variance_ratio_))

# Method 2: Truncated SVD
svd = TruncatedSVD(n_components=2)
X_svd = svd.fit_transform(X_scaled)
print("Explained variance ratio (SVD):", svd.explained_variance_ratio_)
print("Total explained variance (SVD):", sum(svd.explained_variance_ratio_))

# Method 3: t-SNE
tsne = TSNE(n_components=2, random_state=42)
X_tsne = tsne.fit_transform(X_scaled)

# Visualize results
plt.figure(figsize=(18, 6))

# PCA plot
plt.subplot(1, 3, 1)
plt.scatter(X_pca[:, 0], X_pca[:, 1], alpha=0.5)
plt.title('PCA Projection')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')

# SVD plot
plt.subplot(1, 3, 2)
plt.scatter(X_svd[:, 0], X_svd[:, 1], alpha=0.5)
plt.title('SVD Projection')
plt.xlabel('Component 1')
plt.ylabel('Component 2')

# t-SNE plot
plt.subplot(1, 3, 3)
plt.scatter(X_tsne[:, 0], X_tsne[:, 1], alpha=0.5)
plt.title('t-SNE Projection')
plt.xlabel('t-SNE Component 1')
plt.ylabel('t-SNE Component 2')

plt.tight_layout()
plt.show()
                    </code>
                </pre>
                
                <p>This code demonstrates three dimensionality reduction techniques: Principal Component Analysis (PCA), Truncated Singular Value Decomposition (SVD), and t-Distributed Stochastic Neighbor Embedding (t-SNE).</p>
            </div>
        </div>

        <!-- Tab 5: Machine Learning -->
        <div class="tab-content" id="tab5">
            <h2 class="section-title">Machine Learning Implementation</h2>
            
            <div class="content-card">
                <h3 class="subsection-title">Model Selection and Training</h3>
                <p>Train and evaluate different machine learning models:</p>
                
                <pre>
                    <div class="code-header">
                        <span>Model Training</span>
                    </div>
                    <button class="copy-btn"><i class="fas fa-copy"></i> Copy</button>
                    <code class="python">
# Import libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# Load data
df = pd.read_csv('data.csv')
X = df.drop('target', axis=1)  # Replace 'target' with your target column
y = df['target']

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Scale features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Initialize models
models = {
    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),
    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
    'Gradient Boosting': GradientBoostingClassifier(random_state=42),
    'SVM': SVC(probability=True, random_state=42)
}

# Train and evaluate models
results = {}
for name, model in models.items():
    # Train model
    model.fit(X_train_scaled, y_train)
    
    # Make predictions
    y_pred = model.predict(X_test_scaled)
    
    # Evaluate model
    accuracy = accuracy_score(y_test, y_pred)
    results[name] = {
        'model': model,
        'accuracy': accuracy,
        'predictions': y_pred,
        'cross_val': np.mean(cross_val_score(model, X_train_scaled, y_train, cv=5))
    }
    
    print(f"\n{name} Results:")
    print(f"Accuracy: {accuracy:.4f}")
    print(f"Cross-validation Score: {results[name]['cross_val']:.4f}")
    print("\nClassification Report:")
    print(classification_report(y_test, y_pred))

# Compare models
model_names = list(results.keys())
accuracies = [results[name]['accuracy'] for name in model_names]
cv_scores = [results[name]['cross_val'] for name in model_names]

plt.figure(figsize=(12, 6))
x = np.arange(len(model_names))
width = 0.35

plt.bar(x - width/2, accuracies, width, label='Test Accuracy')
plt.bar(x + width/2, cv_scores, width, label='Cross-validation Score')

plt.xlabel('Models')
plt.ylabel('Score')
plt.title('Model Performance Comparison')
plt.xticks(x, model_names, rotation=45)
plt.legend()
plt.tight_layout()
plt.show()

# Plot confusion matrix for best model
best_model_name = max(results, key=lambda x: results[x]['accuracy'])
cm = confusion_matrix(y_test, results[best_model_name]['predictions'])

plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title(f'Confusion Matrix - {best_model_name}')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.tight_layout()
plt.show()
                    </code>
                </pre>
                
                <p>This comprehensive code trains and evaluates multiple machine learning models (Logistic Regression, Random Forest, Gradient Boosting, and SVM), compares their performance using accuracy and cross-validation scores, and visualizes the results.</p>
            </div>
            
            <div class="content-card">
                <h3 class="subsection-title">Hyperparameter Tuning</h3>
                <p>Optimize model performance through hyperparameter tuning:</p>
                
                <pre>
                    <div class="code-header">
                        <span>Hyperparameter Tuning</span>
                    </div>
                    <button class="copy-btn"><i class="fas fa-copy"></i> Copy</button>
                    <code class="python">
# Import libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
import matplotlib.pyplot as plt
import seaborn as sns

# Load data
df = pd.read_csv('data.csv')
X = df.drop('target', axis=1)  # Replace 'target' with your target column
y = df['target']

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Scale features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Method 1: Grid Search
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

grid_search = GridSearchCV(
    RandomForestClassifier(random_state=42),
    param_grid=param_grid,
    cv=5,
    scoring='accuracy',
    n_jobs=-1,
    verbose=1
)

grid_search.fit(X_train_scaled, y_train)

print("Best parameters (Grid Search):", grid_search.best_params_)
print("Best score (Grid Search):", grid_search.best_score_)

# Method 2: Randomized Search (for larger parameter spaces)
from scipy.stats import randint

param_dist = {
    'n_estimators': randint(50, 500),
    'max_depth': randint(10, 50),
    'min_samples_split': randint(2, 20),
    'min_samples_leaf': randint(1, 10),
    'max_features': ['auto', 'sqrt', 'log2']
}

random_search = RandomizedSearchCV(
    RandomForestClassifier(random_state=42),
    param_distributions=param_dist,
    n_iter=20,
    cv=5,
    scoring='accuracy',
    n_jobs=-1,
    random_state=42,
    verbose=1
)

random_search.fit(X_train_scaled, y_train)

print("Best parameters (Random Search):", random_search.best_params_)
print("Best score (Random Search):", random_search.best_score_)

# Train final model with best parameters
best_model = RandomForestClassifier(**grid_search.best_params_, random_state=42)
best_model.fit(X_train_scaled, y_train)
best_accuracy = best_model.score(X_test_scaled, y_test)
print(f"Final model accuracy on test set: {best_accuracy:.4f}")

# Feature importance
feature_importance = best_model.feature_importances_
feature_names = X.columns

# Sort features by importance
sorted_idx = np.argsort(feature_importance)
plt.figure(figsize=(10, 8))
plt.barh(range(len(sorted_idx)), feature_importance[sorted_idx], align='center')
plt.yticks(range(len(sorted_idx)), [feature_names[i] for i in sorted_idx])
plt.title('Feature Importance')
plt.tight_layout()
plt.show()
                    </code>
                </pre>
                
                <p>This code demonstrates hyperparameter tuning using both Grid Search and Randomized Search methods, trains a final model with the best parameters, and visualizes feature importance.</p>
            </div>
            
            <div class="content-card">
                <h3 class="subsection-title">Model Evaluation and Interpretation</h3>
                <p>Evaluate model performance and interpret results:</p>
                
                <pre>
                    <div class="code-header">
                        <span>Model Evaluation</span>
                    </div>
                    <button class="copy-btn"><i class="fas fa-copy"></i> Copy</button>
                    <code class="python">
# Import libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, learning_curve
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score
import matplotlib.pyplot as plt
import seaborn as sns
import shap

# Load data
df = pd.read_csv('data.csv')
X = df.drop('target', axis=1)  # Replace 'target' with your target column
y = df['target']

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Scale features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Train model
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train_scaled, y_train)

# Get predictions and probabilities
y_pred = model.predict(X_test_scaled)
y_prob = model.predict_proba(X_test_scaled)[:, 1]  # Probability of positive class

# 1. ROC Curve
fpr, tpr, thresholds = roc_curve(y_test, y_prob)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(10, 8))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()

# 2. Precision-Recall Curve
precision, recall, _ = precision_recall_curve(y_test, y_prob)
avg_precision = average_precision_score(y_test, y_prob)

plt.figure(figsize=(10, 8))
plt.plot(recall, precision, color='blue', lw=2, label=f'Precision-Recall curve (AP = {avg_precision:.2f})')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.legend(loc="lower left")
plt.show()

# 3. Learning Curve
train_sizes, train_scores, test_scores = learning_curve(
    model, X_train_scaled, y_train, cv=5, n_jobs=-1, 
    train_sizes=np.linspace(0.1, 1.0, 10)
)

train_mean = np.mean(train_scores, axis=1)
train_std = np.std(train_scores, axis=1)
test_mean = np.mean(test_scores, axis=1)
test_std = np.std(test_scores, axis=1)

plt.figure(figsize=(10, 8))
plt.plot(train_sizes, train_mean, color='blue', marker='o', markersize=5, label='Training accuracy')
plt.fill_between(train_sizes, train_mean + train_std, train_mean - train_std, alpha=0.15, color='blue')
plt.plot(train_sizes, test_mean, color='green', marker='s', markersize=5, label='Validation accuracy')
plt.fill_between(train_sizes, test_mean + test_std, test_mean - test_std, alpha=0.15, color='green')
plt.xlabel('Training Set Size')
plt.ylabel('Accuracy')
plt.title('Learning Curve')
plt.legend(loc='lower right')
plt.grid(True)
plt.show()

# 4. SHAP Values for Model Interpretation
# Create a small explainer dataset
explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(X_test_scaled)

# Summary plot
plt.figure(figsize=(10, 8))
shap.summary_plot(shap_values, X_test_scaled, feature_names=X.columns)

# Individual prediction explanation
plt.figure(figsize=(10, 8))
shap.force_plot(explainer.expected_value, shap_values[0,:], X_test_scaled[0,:], feature_names=X.columns)
                    </code>
                </pre>
                
                <p>This comprehensive evaluation code creates ROC curves, precision-recall curves, learning curves, and uses SHAP values to interpret model predictions and feature importance.</p>
            </div>
            
            <div class="content-card">
                <h3 class="subsection-title">Model Deployment</h3>
                <p>Save and deploy your trained machine learning model:</p>
                
                <pre>
                    <div class="code-header">
                        <span>Model Deployment</span>
                    </div>
                    <button class="copy-btn"><i class="fas fa-copy"></i> Copy</button>
                    <code class="python">
# Import libraries
import pandas as pd
import numpy as np
import pickle
import joblib
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from flask import Flask, request, jsonify

# 1. Save model and preprocessing components
model = RandomForestClassifier(n_estimators=100, random_state=42)  # Your trained model
scaler = StandardScaler()  # Your fitted scaler

# Option 1: Save with pickle
with open('model.pkl', 'wb') as f:
    pickle.dump(model, f)
    
with open('scaler.pkl', 'wb') as f:
    pickle.dump(scaler, f)

# Option 2: Save with joblib (better for large NumPy arrays)
joblib.dump(model, 'model.joblib')
joblib.dump(scaler, 'scaler.joblib')

# 2. Load model and preprocessing components
# Option 1: Load with pickle
with open('model.pkl', 'rb') as f:
    loaded_model = pickle.load(f)
    
with open('scaler.pkl', 'rb') as f:
    loaded_scaler = pickle.load(f)

# Option 2: Load with joblib
loaded_model = joblib.load('model.joblib')
loaded_scaler = joblib.load('scaler.joblib')

# 3. Create a simple Flask API for model deployment
app = Flask(__name__)

@app.route('/predict', methods=['POST'])
def predict():
    # Get data from POST request
    data = request.json
    
    # Convert to DataFrame
    df = pd.DataFrame(data, index=[0])
    
    # Preprocess data
    scaled_data = loaded_scaler.transform(df)
    
    # Make prediction
    prediction = loaded_model.predict(scaled_data)
    probability = loaded_model.predict_proba(scaled_data)[:, 1]
    
    # Return prediction
    return jsonify({
        'prediction': int(prediction[0]),
        'probability': float(probability[0])
    })

# Run the API
if __name__ == '__main__':
    app.run(debug=True, host='0.0.0.0', port=5000)

# 4. Example of making a request to the API
"""
import requests
import json

# API endpoint
url = 'http://localhost:5000/predict'

# Sample data
data = {
    'feature1': 0.5,
    'feature2': 1.0,
    'feature3': -0.5
}

# Make request
response = requests.post(url, json=data)

# Print result
print(response.json())
"""
                    </code>
                </pre>
                
                <p>This code demonstrates how to save and load machine learning models using pickle and joblib, and how to create a simple Flask API for model deployment.</p>
            </div>
        </div>
    </main>

    <!-- Back to top button -->
    <div class="back-to-top" id="backToTop">
        <i class="fas fa-arrow-up"></i>
    </div>

    <!-- Toast Notification -->
    <div id="toast" class="toast">Code copied to clipboard!</div>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.5.0/highlight.min.js"></script>
    <script>
        // Initialize syntax highlighting
        document.addEventListener('DOMContentLoaded', function() {
            hljs.highlightAll();
            
            // Add copy functionality to code blocks
            document.querySelectorAll('.copy-btn').forEach(button => {
                button.addEventListener('click', function() {
                    const codeBlock = this.parentNode.querySelector('code');
                    const textToCopy = codeBlock.textContent;
                    
                    navigator.clipboard.writeText(textToCopy).then(() => {
                        const toast = document.getElementById('toast');
                        toast.style.display = 'block';
                        setTimeout(() => {
                            toast.style.display = 'none';
                        }, 2000);
                    }).catch(err => {
                        console.error('Failed to copy: ', err);
                    });
                });
            });
            
            // Tab functionality
            const tabButtons = document.querySelectorAll('.tab-button');
            const tabContents = document.querySelectorAll('.tab-content');
            
            tabButtons.forEach(button => {
                button.addEventListener('click', () => {
                    const tabId = button.getAttribute('data-tab');
                    
                    // Remove active class from all buttons and contents
                    tabButtons.forEach(btn => btn.classList.remove('active'));
                    tabContents.forEach(content => content.classList.remove('active'));
                    
                    // Add active class to current button and content
                    button.classList.add('active');
                    document.getElementById(tabId).classList.add('active');
                });
            });
            
            // Dark mode toggle
            const themeToggle = document.getElementById('themeToggle');
            const body = document.body;
            const icon = themeToggle.querySelector('i');
            
            themeToggle.addEventListener('click', () => {
                body.classList.toggle('dark-theme');
                
                if (body.classList.contains('dark-theme')) {
                    icon.classList.remove('fa-moon');
                    icon.classList.add('fa-sun');
                } else {
                    icon.classList.remove('fa-sun');
                    icon.classList.add('fa-moon');
                }
            });

            // Back to top button functionality
            const backToTopButton = document.getElementById('backToTop');
            
            window.addEventListener('scroll', () => {
                if (window.pageYOffset > 300) {
                    backToTopButton.classList.add('visible');
                } else {
                    backToTopButton.classList.remove('visible');
                }
            });
            
            backToTopButton.addEventListener('click', () => {
                window.scrollTo({
                    top: 0,
                    behavior: 'smooth'
                });
            });
        });
    </script>
</body>
</html>
